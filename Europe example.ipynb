{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MagPySV example workflow - European observatories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup python paths and import some modules\n",
    "from IPython.display import Image\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Import all of the MagPySV modules\n",
    "import magpysv.denoise as denoise\n",
    "import magpysv.io as io\n",
    "import magpysv.model_prediction as model_prediction\n",
    "import magpysv.plots as plots\n",
    "import magpysv.tools as tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gmdata_webinterface import consume_webservices as cws\n",
    "\n",
    "# Required dataset - only the hourly WDC dataset is currently supported \n",
    "cadence = 'hour'\n",
    "service = 'WDC'\n",
    "\n",
    "# Start and end dates of the data download\n",
    "start_date = dt.date(1960, 1, 1)\n",
    "end_date = dt.date(2010, 12, 31)\n",
    "\n",
    "# Observatories of interest\n",
    "observatory_list = ['CLF', 'NGK', 'WNG']\n",
    "\n",
    "# Output path for data\n",
    "download_dir = 'data'\n",
    "\n",
    "cws.fetch_data(start_date= start_date, end_date=end_date,\n",
    "        station_list=observatory_list, cadence=cadence,\n",
    "        service=service, saveroot=download_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all data from the WDC files, convert into the proper hourly means using the tabular base and save the X, Y and Z components to CSV files.  This may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dir = os.path.join(download_dir, 'hourly')\n",
    "io.wdc_to_hourly_csv(wdc_path=download_dir, write_dir=write_dir, obs_list=observatory_list,\n",
    "                  print_obs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to file containing baseline discontinuity information\n",
    "baseline_data = tools.get_baseline_info(fname='baseline_records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all observatories and calculate SV series as first differences of monthly means (FDMM) for each\n",
    "for observatory in observatory_list:\n",
    "    print(observatory)\n",
    "    # Load hourly data\n",
    "    data_file = observatory + '.csv'\n",
    "    hourly_data = io.read_csv_data(\n",
    "        fname=os.path.join(download_dir, 'hourly', data_file),\n",
    "        data_type='mf')\n",
    "    # Resample to monthly means\n",
    "    resampled_field_data = tools.data_resampling(hourly_data, sampling='MS', average_date=True)\n",
    "    # Correct documented baseline changes\n",
    "    tools.correct_baseline_change(observatory=observatory,\n",
    "                          field_data=resampled_field_data,\n",
    "                          baseline_data=baseline_data, print_data=True)\n",
    "    # Write out the monthly means for magnetic field\n",
    "    io.write_csv_data(data=resampled_field_data,\n",
    "                            write_dir=os.path.join(download_dir, 'monthly_mf'),\n",
    "                            obs_name=observatory)\n",
    "    # Calculate SV from monthly field means\n",
    "    sv_data = tools.calculate_sv(resampled_field_data,\n",
    "                                   mean_spacing=1)\n",
    "    # Write out the SV data\n",
    "    io.write_csv_data(data=sv_data,\n",
    "                               write_dir=os.path.join(download_dir, 'monthly_sv', 'fdmm'),\n",
    "                               obs_name=observatory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate the data for our selected observatories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the Setup section, everything preceding this cell only needs to be run once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Observatories of interest\n",
    "observatory_list = ['CLF', 'NGK', 'WNG']\n",
    "\n",
    "# Where the data are stored\n",
    "download_dir = 'data'\n",
    "\n",
    "# Start and end dates of the analysis as (year, month, day)\n",
    "start = dt.datetime(1960, 1, 1)\n",
    "end = dt.datetime(2010, 12, 31)\n",
    "\n",
    "obs_data, model_sv_data, model_mf_data = io.combine_csv_data(\n",
    "    start_date=start, end_date=end, obs_list=observatory_list,\n",
    "    data_path=os.path.join(download_dir, 'monthly_sv', 'fdmm'),\n",
    "    model_path='model_predictions', day_of_month=1)\n",
    "\n",
    "dates = obs_data['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "obs_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SV plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for observatory in observatory_list:\n",
    "    fig = plots.plot_sv(dates=dates, sv=obs_data.filter(regex=observatory),\n",
    "                    model=model_sv_data.filter(regex=observatory),\n",
    "                    fig_size=(6, 6), font_size=10, label_size=16, plot_legend=False,\n",
    "                    obs=observatory, model_name='COV-OBS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate SV residuals, we need SV predictions from a geomagnetic field model. This example uses output from the COV-OBS model by Gillet et al. (2013, Geochem. Geophys. Geosyst.,\n",
    "https://doi.org/10.1002/ggge.20041; 2015, Earth, Planets and Space,\n",
    "https://doi.org/10.1186/s40623-015-0225-z2013) to obtain model\n",
    "predictions for these observatory locations. The code can be obtained from\n",
    "http://www.spacecenter.dk/files/magnetic-models/COV-OBSx1/ and no modifications\n",
    "are necessary to run it using functions found MagPySV's model_prediction module. For convenience, model output for the locations used in this notebook are included in the examples directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = tools.calculate_residuals(obs_data=obs_data, model_data=model_sv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sv_data.drop(['date'], axis=1, inplace=True)\n",
    "obs_data.drop(['date'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# External noise removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute covariance matrix of the residuals (for all observatories combined) and its eigenvalues and eigenvectors. Since the residuals represent signals present in the data, but not the internal field model, we use them to find a proxy for external magnetic fields (Wardinski & Holme, 2011, GJI, https://doi.org/10.1111/j.1365-246X.2011.04988.x). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoised, proxy, eigenvals, eigenvecs, projected_residuals, corrected_residuals = denoise.eigenvalue_analysis(\n",
    "    dates=dates, obs_data=obs_data, model_data=model_sv_data, residuals=residuals,\n",
    "    proxy_number=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoised SV plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots showing the original SV data, the denoised data (optionally with a running average) and the field model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for observatory in observatory_list:\n",
    "    xratio, yratio, zratio = plots.plot_sv_comparison(dates=dates, denoised_sv=denoised.filter(regex=observatory),\n",
    "        residuals=residuals.filter(regex=observatory),\n",
    "        corrected_residuals = corrected_residuals.filter(regex=observatory),\n",
    "        noisy_sv=obs_data.filter(regex=observatory), model=model_sv_data.filter(regex=observatory),\n",
    "        model_name='COV-OBS', fig_size=(6, 6), font_size=10, label_size=14, obs=observatory, plot_rms=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots showing the denoised data (optionally with a running average) and the field model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for observatory in observatory_list:\n",
    "    plots.plot_sv(dates=dates, sv=denoised.filter(regex=observatory), model=model_sv_data.filter(regex=observatory),\n",
    "                  fig_size=(6, 6), font_size=10, label_size=14, plot_legend=False, obs=observatory,\n",
    "                  model_name='COV-OBS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot proxy signal, eigenvalues and eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the proxy signal used to denoise the data with the Dst index, measures the intensity of the equatorial electrojet (the \"ring current\"). Files for the ap (ap_fdmm.csv) and AE (ae_fdmm.csv) are also included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plots.plot_index_dft(index_file='index_data/dst_fdmm.csv', dates=denoised.date, signal=proxy, fig_size=(6, 6), font_size=10,\n",
    "                       label_size=14, plot_legend=True, index_name='Dst')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the eigenvalues of the covariance matrix of the residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_eigenvalues(values=eigenvals, font_size=12, label_size=16, fig_size=(6, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the eigenvectors corresponding to the three largest eigenvalues. The noisiest direction (used to denoise in this example) is mostly X, with some Z, which is consistent with the ring current for European observatories. The second noisiest direction (also used to denoise in this example) is predominantly Z, with some X, and has a large semi-annual contribution that is likely of external origin. However, the third noisiest direction is a coherent Y signal across Europe, which does not correspond to a known direction of external signal. We did not remove this direction during denoising as it could be a real internal field variation that is not captured by the field model. However, its DFT shows a significant semi-annual contribution so this eigendircetion is likely to be in part of external origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plots.plot_eigenvectors(obs_names=observatory_list, eigenvecs=eigenvecs[:,0:3], fig_size=(6, 4),\n",
    "                          font_size=10, label_size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove remaining spikes in the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "denoised.drop(['date'], axis=1, inplace=True)\n",
    "for column in denoised:\n",
    "    denoised[column] = denoise.detect_outliers(dates=dates, signal=denoised[column], obs_name=column, threshold=5,\n",
    "                                               window_length=120, plot_fig=False, fig_size=(10, 3), font_size=10, label_size=14)\n",
    "denoised.insert(0, 'date', dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write denoised data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for observatory in observatory_list:\n",
    "    print(observatory)\n",
    "    sv_data=denoised.filter(regex=observatory)\n",
    "    sv_data.insert(0, 'date', dates)\n",
    "    sv_data.columns = [\"date\", \"dX\", \"dY\", \"dZ\"]\n",
    "    io.write_csv_data(data=sv_data, write_dir=os.path.join(download_dir, 'denoised', 'european'),\n",
    "                               obs_name=observatory, decimal_dates=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Averaging data over Europe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select denoised data for each SV component at all observatories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_X = denoised.filter(regex='dX')\n",
    "model_X = model_sv_data.filter(regex='dX')\n",
    "obs_Y = denoised.filter(regex='dY')\n",
    "model_Y = model_sv_data.filter(regex='dY')\n",
    "obs_Z = denoised.filter(regex='dZ')\n",
    "model_Z = model_sv_data.filter(regex='dZ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average data and model for each component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_X = pd.DataFrame(np.mean(obs_X.values, axis=1))\n",
    "mean_X.columns = ['dX']\n",
    "mean_model_X = np.mean(model_X, axis=1)\n",
    "mean_Y = pd.DataFrame(np.mean(obs_Y.values, axis=1))\n",
    "mean_Y.columns = ['dY']\n",
    "mean_model_Y = np.mean(model_Y, axis=1)\n",
    "mean_Z = pd.DataFrame(np.mean(obs_Z.values, axis=1))\n",
    "mean_Z.columns = ['dZ']\n",
    "mean_model_Z = np.mean(model_Z, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove outliers from averaged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_X = denoise.detect_outliers(dates=dates, signal=mean_X, obs_name='X', threshold=2.5,\n",
    "                                               window_length=72, plot_fig=False, fig_size=(10, 3), font_size=10, label_size=14)\n",
    "mean_Y = denoise.detect_outliers(dates=dates, signal=mean_Y, obs_name='Y', threshold=2.5,\n",
    "                                               window_length=72, plot_fig=False, fig_size=(10, 3), font_size=10, label_size=14)\n",
    "mean_Z = denoise.detect_outliers(dates=dates, signal=mean_Z, obs_name='Z', threshold=2.5,\n",
    "                                               window_length=72, plot_fig=False, fig_size=(10, 3), font_size=10, label_size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at model predictions for all observatories, and the averaged model, to see if the average is representative of the trend at all locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(dates, model_X)\n",
    "plt.plot(dates, mean_model_X, 'k--')\n",
    "plt.legend(['CLF', 'NGK', 'WNG', 'Average'], frameon=False, fontsize=10, loc=(0.1,1.04), ncol=4)\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(dates, model_Y)\n",
    "plt.plot(dates, mean_model_Y, 'k--')\n",
    "plt.ylabel('SV (nT/yr)',  fontsize=14)\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(dates, model_Z)\n",
    "plt.plot(dates, mean_model_Z, 'k--')\n",
    "plt.xlabel('Year',  fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the averaged data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(dates, mean_X, 'b')\n",
    "plt.plot(dates, np.mean(model_X, axis=1), 'r')\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(dates, mean_Y, 'b')\n",
    "plt.plot(dates, np.mean(model_Y, axis=1), 'r')\n",
    "plt.ylabel('SV (nT/yr)', fontsize=14)\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(dates, mean_Z, 'b', label='Averaged data')\n",
    "plt.plot(dates, np.mean(model_Z, axis=1), 'r', label='Averaged COV-OBS')\n",
    "plt.xlabel('Year',  fontsize=14)\n",
    "plt.legend(loc='best', fontsize=10, frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data selection using the ap index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select an observatory, load its hourly magnetic field data and correct documented baseline changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observatory = 'CLF'\n",
    "data_file = observatory + '.csv'\n",
    "\n",
    "hourly_data = io.read_csv_data(\n",
    "    fname=os.path.join(download_dir, 'hourly', data_file),\n",
    "    data_type='mf')\n",
    "\n",
    "# Path to file containing baseline discontinuity information\n",
    "baseline_data = tools.get_baseline_info(fname='baseline_records')\n",
    "\n",
    "# Correct documented baseline changes\n",
    "tools.correct_baseline_change(observatory=observatory,\n",
    "                      field_data=hourly_data,\n",
    "                      baseline_data=baseline_data, print_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply an ap criterion to discard noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discard hours with ap > threshold\n",
    "ap_hourly_applied = tools.apply_Ap_threshold(obs_data=hourly_data, Ap_file=os.path.join('index_data', 'ap_hourly.csv'),\n",
    "                               threshold=7.0)\n",
    "\n",
    "# Discard days with Ap > threshold (where Ap is the daily average of the 3-hourly ap values)\n",
    "ap_daily_applied = tools.apply_Ap_threshold(obs_data=hourly_data, Ap_file=os.path.join('index_data', 'ap_daily.csv'),\n",
    "                               threshold=7.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the percentage of data remaining after applying the ap threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Hourly ap threshold applied: ', ap_hourly_applied.X.count()/hourly_data.X.count() * 100, '% remaining')\n",
    "print('Daily Ap threshold applied: ', ap_daily_applied.X.count()/hourly_data.X.count() * 100, '% remaining')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the hourly magnetic field data before and after appyling the ap threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(hourly_data.date, hourly_data.X, 'b')\n",
    "plt.plot(hourly_data.date, ap_hourly_applied.X, 'r')\n",
    "plt.plot(hourly_data.date, ap_daily_applied.X, 'c')\n",
    "plt.xlim([dt.date(1960, 1, 1), dt.date(2010, 1, 1)])\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(hourly_data.date, hourly_data.Y, 'b')\n",
    "plt.plot(hourly_data.date, ap_hourly_applied.Y, 'r')\n",
    "plt.plot(hourly_data.date, ap_daily_applied.Y, 'c')\n",
    "plt.xlim([dt.date(1960, 1, 1), dt.date(2010, 1, 1)])\n",
    "plt.ylabel('Magnetic Field (nT)', fontsize=16)\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(hourly_data.date, hourly_data.Z, 'b', label='All data')\n",
    "plt.plot(hourly_data.date, ap_hourly_applied.Z, 'r', label='ap ≤ 7')\n",
    "plt.plot(hourly_data.date, ap_daily_applied.Z, 'c', label='Ap ≤ 7')\n",
    "plt.xlim([dt.date(1960, 1, 1), dt.date(2010, 1, 1)])\n",
    "plt.xlabel('Year', fontsize=16)\n",
    "plt.legend(frameon=False)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d = hourly_data['date']\n",
    "hourly_data.drop(['date'], axis=1, inplace=True)\n",
    "for column in hourly_data:\n",
    "    hourly_data[column] = denoise.detect_outliers(dates=d, signal=hourly_data[column], obs_name=column, threshold=10,\n",
    "                                                  signal_type='MF', window_length=24*365*10, plot_fig=True,\n",
    "                                                  fig_size=(7, 4), font_size=10, label_size=14)\n",
    "hourly_data.insert(0, 'date', d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the SV obtained when calculated using all hourly data and hourly the ap threshold applied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing FDMM and ADMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the hourly data above to monthly means\n",
    "resampled_field_data = tools.data_resampling(hourly_data, sampling='MS', average_date=True)\n",
    "\n",
    "# Calculate SV from monthly field means\n",
    "sv_fdmm = tools.calculate_sv(resampled_field_data,\n",
    "                               mean_spacing=1)\n",
    "sv_admm = tools.calculate_sv(resampled_field_data,\n",
    "                               mean_spacing=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the SV calculated as FDMM and ADMM\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(sv_fdmm.date, sv_fdmm.dx, 'b')\n",
    "plt.plot(sv_admm.date, sv_admm.dx, 'r')\n",
    "plt.xlim([dt.date(1960, 1, 1), dt.date(2010, 1, 1)])\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(sv_fdmm.date, sv_fdmm.dy, 'b')\n",
    "plt.plot(sv_admm.date, sv_admm.dy, 'r')\n",
    "plt.xlim([dt.date(1960, 1, 1), dt.date(2010, 1, 1)])\n",
    "plt.ylabel('SV (nT/yr)', fontsize=16)\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(sv_fdmm.date, sv_fdmm.dz, 'b', label='FDMM')\n",
    "plt.plot(sv_admm.date, sv_admm.dz, 'r', label = 'ADMM')\n",
    "plt.xlim([dt.date(1960, 1, 1), dt.date(2010, 1, 1)])\n",
    "plt.gca().xaxis_date()\n",
    "plt.xlabel('Year', fontsize=16)\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
