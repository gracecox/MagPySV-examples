{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MagPySV example workflow - high latitude observatories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup python paths and import some modules\n",
    "from IPython.display import Image\n",
    "import sys\n",
    "import os\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "# Import all of the MagPySV modules\n",
    "import magpysv.denoise as denoise\n",
    "import magpysv.io as io\n",
    "import magpysv.model_prediction as model_prediction\n",
    "import magpysv.plots as plots\n",
    "import magpysv.tools as tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gmdata_webinterface import consume_webservices as cws\n",
    "\n",
    "# Required dataset - only the hourly WDC dataset is currently supported \n",
    "cadence = 'hour'\n",
    "service = 'WDC'\n",
    "\n",
    "# Start and end dates of the data download\n",
    "start_date = dt.date(1980, 1, 1)\n",
    "end_date = dt.date(2010, 12, 31)\n",
    "\n",
    "# Observatories of interest\n",
    "observatory_list = ['BLC', 'BRW', 'MBC', 'OTT', 'RES', 'STJ', 'THL', 'VIC', 'YKC']\n",
    "\n",
    "# Output path for data\n",
    "download_dir = 'data'\n",
    "\n",
    "cws.fetch_data(start_date= start_date, end_date=end_date,\n",
    "        station_list=observatory_list, cadence=cadence,\n",
    "        service=service, saveroot=download_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all data from the WDC files, convert into the proper hourly means using the tabular base and save the X, Y and Z components to CSV files. This may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io.wdc_to_hourly_csv(wdc_path=download_dir, write_dir=os.path.join(download_dir, 'hourly'), obs_list=observatory_list,\n",
    "                  print_obs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to file containing baseline discontinuity information\n",
    "baseline_data = tools.get_baseline_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all observatories and calculate SV series as annual differences of monthly means (ADMM) for each\n",
    "for observatory in observatory_list:\n",
    "    print(observatory)\n",
    "    # Load hourly data\n",
    "    data_file = observatory + '.csv'\n",
    "    hourly_data = io.read_csv_data(\n",
    "        fname=os.path.join(download_dir, 'hourly', data_file),\n",
    "        data_type='mf')\n",
    "\n",
    "    # Discard days with Ap > threshold (where Ap is the daily average of the 3-hourly ap values) - optional,\n",
    "    # uncomment the next two lines\n",
    "#    hourly_data = tools.apply_Ap_threshold(obs_data=hourly_data, Ap_file=os.path.join('index_data', 'ap_daily.csv'),\n",
    "#                               threshold=30.0)\n",
    "\n",
    "    # Resample to monthly means\n",
    "    resampled_field_data = tools.data_resampling(hourly_data, sampling='MS', average_date=True)\n",
    "    # Correct documented baseline changes\n",
    "    tools.correct_baseline_change(observatory=observatory,\n",
    "                          field_data=resampled_field_data,\n",
    "                          baseline_data=baseline_data, print_data=True)\n",
    "    # Write out the monthly means for magnetic field\n",
    "    io.write_csv_data(data=resampled_field_data,\n",
    "                            write_dir=os.path.join(download_dir, 'monthly_mf'),\n",
    "                            obs_name=observatory)\n",
    "    # Calculate SV from monthly field means\n",
    "    sv_data = tools.calculate_sv(resampled_field_data,\n",
    "                                   mean_spacing=12)\n",
    "    # Write out the SV data\n",
    "    io.write_csv_data(data=sv_data,\n",
    "                               write_dir=os.path.join(download_dir, 'monthly_sv', 'admm'),\n",
    "                               obs_name=observatory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High latitude regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"zonemap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerun the analysis below for each of the three high latitude regions. Besides the Setup section, everything preceding this cell only needs be run only once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate the data for our selected observatories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select observatories in one high latitude region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observatory_list = ['MBC', 'RES', 'THL'] # Polar cap\n",
    "#observatory_list = ['BLC', 'BRW', 'YKC'] # Auroral zone\n",
    "#observatory_list = ['OTT', 'STJ', 'VIC'] # Sub-auroral zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate the data for our selected observatories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Where the data are stored\n",
    "download_dir = 'data'\n",
    "\n",
    "# Start and end dates of the analysis as (year, month, day)\n",
    "start = dt.datetime(1980, 1, 1)\n",
    "end = dt.datetime(2010, 12, 31)\n",
    "\n",
    "obs_data, model_sv_data, model_mf_data = io.combine_csv_data(\n",
    "    start_date=start, end_date=end, obs_list=observatory_list,\n",
    "    data_path=os.path.join(download_dir, 'monthly_sv', 'admm'),\n",
    "    model_path='model_predictions', day_of_month=15)\n",
    "\n",
    "dates = obs_data['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "obs_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SV plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for observatory in observatory_list:\n",
    "    fig = plots.plot_sv(dates=dates, sv=obs_data.filter(regex=observatory),\n",
    "                    model=model_sv_data.filter(regex=observatory),\n",
    "                    fig_size=(6, 6), font_size=10, label_size=16, plot_legend=False,\n",
    "                    obs=observatory, model_name='COV-OBS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Optionally remove spikes in the data before denoising. Large outliers can affect the denoising process so better to remove them beforehand for some series (i.e. at high latitude observatories). Try changing the threshold or window length to see how this affects which points are identified as outliers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "obs_data.drop(['date'], axis=1, inplace=True)\n",
    "for column in obs_data:    \n",
    "    obs_data[column] = denoise.detect_outliers(dates=dates, signal=obs_data[column], obs_name=column,\n",
    "                                               threshold=4,\n",
    "                                               window_length=120, plot_fig=True, fig_size=(10,3))\n",
    "obs_data.insert(0, 'date', dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate SV residuals, we need SV predictions from a geomagnetic field model. This example uses output from the COV-OBS model by Gillet et al. (2013, Geochem. Geophys. Geosyst.,\n",
    "https://doi.org/10.1002/ggge.20041; 2015, Earth, Planets and Space,\n",
    "https://doi.org/10.1186/s40623-015-0225-z2013) to obtain model\n",
    "predictions for these observatory locations. The code can be obtained from\n",
    "http://www.spacecenter.dk/files/magnetic-models/COV-OBSx1/ and no modifications\n",
    "are necessary to run it using functions found MagPySV's model_prediction module. For convenience, model output for the locations used in this notebook are included in the examples directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = tools.calculate_residuals(obs_data=obs_data, model_data=model_sv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sv_data.drop(['date'], axis=1, inplace=True)\n",
    "obs_data.drop(['date'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# External noise removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute covariance matrix of the residuals (for all observatories combined) and its eigenvalues and eigenvectors. Since the residuals represent signals present in the data, but not the internal field model, we use them to find a proxy for external magnetic fields (Wardinski & Holme, 2011, GJI, https://doi.org/10.1111/j.1365-246X.2011.04988.x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoised, proxy, eigenvals, eigenvecs, projected_residuals, corrected_residuals, cov_mat = denoise.eigenvalue_analysis(\n",
    "    dates=dates, obs_data=obs_data, model_data=model_sv_data, residuals=residuals,\n",
    "    proxy_number=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoised SV plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots showing the original SV data, the denoised data (optionally with a running average) and the field model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for observatory in observatory_list:\n",
    "    xratio, yratio, zratio = plots.plot_sv_comparison(dates=dates, denoised_sv=denoised.filter(regex=observatory),\n",
    "        residuals=residuals.filter(regex=observatory),\n",
    "        corrected_residuals = corrected_residuals.filter(regex=observatory),\n",
    "        noisy_sv=obs_data.filter(regex=observatory), model=model_sv_data.filter(regex=observatory),\n",
    "        model_name='COV-OBS',\n",
    "        fig_size=(6,6), font_size=10, label_size=14, obs=observatory, plot_rms=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots showing the denoised data (optionally with a running average) and the field model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for observatory in observatory_list:\n",
    "    plots.plot_sv(dates=dates, sv=denoised.filter(regex=observatory), model=model_sv_data.filter(regex=observatory),\n",
    "                  fig_size=(6, 6), font_size=10, label_size=14, plot_legend=False, obs=observatory,\n",
    "                  model_name='COV-OBS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot proxy signal, eigenvalues and eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the proxy signal used to denoise the data with a geomagnetic index at the same temporal resolution. Dst measures the intensity of the equatorial electrojet (the \"ring current\"). AE measures the intensity of the auroral electrojet. Files included with this notebook for annual differences: dst_admm.csv, ap_admm_dst and ae_admm.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plots.plot_index_dft(index_file=os.path.join('index_data', 'dst_admm.csv'), dates=denoised.date, signal=proxy.astype('float'),\n",
    "                     fig_size=(6, 6), font_size=10, label_size=14, plot_legend=True, index_name='Dst')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the eigenvalues of the covariance matrix of the residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plots.plot_eigenvalues(values=eigenvals, font_size=12, label_size=16, fig_size=(6, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the eigenvectors corresponding to the three largest eigenvalues. The noisiest direction (v_000, used to denoise in this example) is mostly:\n",
    "Z in the polar region (no strong correlation with Dst, AE or Dst indices), X and Z in auroral zone (correlates with the AE index) and X and Z in the sub-auroral zone (correlates with the Dst index, similar to European observatories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plots.plot_eigenvectors(obs_names=observatory_list, eigenvecs=eigenvecs[:,0:3], fig_size=(6, 4),\n",
    "                          font_size=10, label_size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove remaining spikes in the time series (if needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "denoised.drop(['date'], axis=1, inplace=True)\n",
    "for column in denoised:\n",
    "    denoised[column] = denoise.detect_outliers(dates=dates, signal=denoised[column], obs_name=column, threshold=5,\n",
    "                                               window_length=120, plot_fig=False, fig_size=(10, 3), font_size=10,\n",
    "                                               label_size=14)\n",
    "denoised.insert(0, 'date', dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write denoised data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for observatory in observatory_list:\n",
    "    print(observatory)\n",
    "    sv_data=denoised.filter(regex=observatory)\n",
    "    sv_data.insert(0, 'date', dates)\n",
    "    sv_data.columns = [\"date\", \"dX\", \"dY\", \"dZ\"]\n",
    "    io.write_csv_data(data=sv_data, write_dir=os.path.join(download_dir, 'denoised', 'highlat'),\n",
    "                               obs_name=observatory, decimal_dates=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
